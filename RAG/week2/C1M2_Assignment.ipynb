{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Implementing Retriever Functions in a RAG System\n",
    "\n",
    "---\n",
    "\n",
    "In this assignment, you will enhance your RAG system by implementing various retrieval functions. Your main tasks will include integrating semantic search and BM25 algorithms as retrieval methods. By the end of this assignment, you will be able to run and evaluate your RAG system with and without these retrieval functions to observe how each one affects performance and improves the quality of the generated answers.\n",
    "\n",
    "In this assignment, you will:\n",
    "\n",
    "* Use a library to implement BM25 search\n",
    "* Implement semantic search using vector embeddings\n",
    "* Implement the Reciprocal Rank Fusion algorithm to combine BM25 and semantic search\n",
    "* Analyze how different retrieval methods impact the responses generated by the LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Importing the libraries](#1)\n",
    "- [ 2 - Loading the Dataset](#2)\n",
    "- [ 3 - Retrieve Functions](#3)\n",
    "  - [ 3.1 Query news by index](#3-1)\n",
    "  - [ 3.2 BM25 Retrieve](#3-2)\n",
    "    - [ Exercise 1](#ex01)\n",
    "  - [ 3.3 Semantic Search](#3-3)\n",
    "  - [ 3.4 Embeddings](#3-4)\n",
    "    - [ Exercise 2](#ex02)\n",
    "  - [ 3.5 RRF Retrieve](#3-5)\n",
    "    - [ Exercise 3](#ex03)\n",
    "- [ 4 - Completing the RAG System](#4)\n",
    "  - [ 4.1 Creating the final prompt](#4-1)\n",
    "  - [ 4.2 Experimenting with the RAG system](#4-2)\n",
    "  - [ 4.3 Ask yourself](#4-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h4 style=\"color:black; font-weight:bold;\">USING THE TABLE OF CONTENTS</h4>\n",
    "\n",
    "JupyterLab provides an easy way for you to navigate through your assignment. It's located under the Table of Contents tab, found in the left panel, as shown in the picture below.\n",
    "\n",
    "![TOC Location](images/toc.png)\n",
    "\n",
    "---\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "- All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "- You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "- Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "- - To submit your notebook for grading, first save it by clicking the 💾 icon on the top left of the page and then click on the <span style=\"background-color: blue; color: white; padding: 3px 5px; font-size: 16px; border-radius: 5px;\">Submit assignment</span> button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Importing the libraries\n",
    "---\n",
    "\n",
    "Alright, let's get started by importing all of the necessary libraries needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import bm25s\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    read_dataframe,\n",
    "    pprint, \n",
    "    generate_with_single_input, \n",
    "    cosine_similarity,\n",
    "    display_widget\n",
    ")\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Loading the Dataset\n",
    "---\n",
    "You will be working with the same Kaggle [BBC News dataset](https://www.kaggle.com/datasets/gpreda/bbc-news) as in Module 1. However, now you will focus on the retrieval part, implementing three different retrieval algorithms and experimenting with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEWS_DATA = read_dataframe(\"news_data_dedup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guid': '18ba9f2676859f393a271d15692a9c6e',\n",
      " 'title': 'WATCH: Would you pay a tourist fee to enter Venice?',\n",
      " 'description': 'From Thursday visitors making a trip to the famous city at '\n",
      "                'peak times will be charged a trial entrance fee.',\n",
      " 'venue': 'BBC',\n",
      " 'url': 'https://www.bbc.co.uk/news/world-europe-68898441',\n",
      " 'published_at': '2024-04-25',\n",
      " 'updated_at': '2024-04-26'}\n"
     ]
    }
   ],
   "source": [
    "pprint(NEWS_DATA[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Retrieve Functions\n",
    "---\n",
    "In this assignment, you will focus on the retrieve part, so the other functions in the RAG system you saw previously will be given.\n",
    "\n",
    "In RAG systems, as you saw in the lectures,the retrieve function is key to finding relevant information from a large set of documents. This step is fundamental to select the best documents to answer a specific query. \n",
    "\n",
    "**Retrieve Functions in RAG:**  \n",
    "As you saw in the lectures, there are several retrieve algorithms used in RAG, in this assignment\n",
    "\n",
    "**Semantic Search vs. BM25 Retrieve:**\n",
    "\n",
    "1. **Semantic Search:**  \n",
    "   This method uses advanced techniques to understand the meaning behind words in a query. Instead of just matching keywords, it looks at the context and relationships between words to find the best matches.\n",
    "\n",
    "2. **BM25 Retrieve:**  \n",
    "   BM25 is a traditional yet effective algorithm that scores documents based on how well they match a query. It looks at factors like how often a term appears in a document, how unique the term is, and the document's length. This helps in efficiently finding documents that are most relevant to the query.\n",
    "\n",
    "In short, semantic search focuses on understanding the meaning of queries, while BM25 provides a reliable way to rank and retrieve documents, making both useful in RAG systems.\n",
    "\n",
    "In this assignment, you will focus in this part:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/retriever_overview.png\" alt=\"RAG Overview\" width=\"60%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-1'></a>\n",
    "### 3.1 Query news by index\n",
    "\n",
    "This function was used previously as a helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_news(indices):\n",
    "    \"\"\"\n",
    "    Retrieves elements from a dataset based on specified indices.\n",
    "\n",
    "    Parameters:\n",
    "    indices (list of int): A list containing the indices of the desired elements in the dataset.\n",
    "    dataset (list or sequence): The dataset from which elements are to be retrieved. It should support indexing.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of elements from the dataset corresponding to the indices provided in list_of_indices.\n",
    "    \"\"\"\n",
    "     \n",
    "    output = [NEWS_DATA[index] for index in indices]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-2'></a>\n",
    "### 3.2 BM25 Retrieve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of BM25 retrieve\n",
    "\n",
    "Let's have an example of BM25 retrieve using the [bm25s](https://bm25s.github.io/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: What are the recent news about GDP?\n",
      "\n",
      "Document retrieved 752 : GDP and the Dow Are Up. But What About American Well-Being? The standard ways of measuring economic growth don’t capture what life is like for real people. A new metric offers a better alternative, especially for seeing disparities across the country.\n",
      "\n",
      "Document retrieved 673 : What the GDP Report Says About Inflation: A Hot First Quarter Thursday’s gross domestic product report suggests that a widely watched inflation reading due Friday could be worse than expected.\n",
      "\n",
      "Document retrieved 289 : A GDP Warning as Signs of Stagflation Appear Slower growth and persistent inflation explain why voters feel glum about the economy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The corpus used will be the title appended with the description\n",
    "corpus = [x['title'] + \" \" + x['description'] for x in NEWS_DATA]\n",
    "\n",
    "# Instantiate the retriever by passing the corpus data\n",
    "BM25_RETRIEVER = bm25s.BM25(corpus=corpus)\n",
    "\n",
    "# Tokenize the chunks\n",
    "tokenized_data = bm25s.tokenize(corpus)\n",
    "\n",
    "# Index the tokenized chunks within the retriever\n",
    "BM25_RETRIEVER.index(tokenized_data)\n",
    "\n",
    "# Tokenize the same query used in the previous exercise\n",
    "sample_query = \"What are the recent news about GDP?\"\n",
    "tokenized_sample_query = bm25s.tokenize(sample_query)\n",
    "\n",
    "# Get the retrieved results and their respective scores\n",
    "results, scores = BM25_RETRIEVER.retrieve(tokenized_sample_query, k=3)\n",
    "\n",
    "print(f\"Results for query: {sample_query}\\n\")\n",
    "for doc in results[0]:\n",
    "  print(f\"Document retrieved {corpus.index(doc)} : {doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex01'></a>\n",
    "\n",
    "<a id='ex01'></a>\n",
    "### Exercise 1\n",
    "\n",
    "In this exercise, you will implement a BM25 retrieval function. This function will take two parameters:\n",
    "\n",
    "* `query`: the search term or phrase you're interested in.\n",
    "* `top_k`: the number of top relevant results you want to retrieve.\n",
    "\n",
    "Your task is to use the BM25 algorithm to find the most relevant documents from a corpus based on the given query. You may refer back to the code above to help complete this exercise.\n",
    "\n",
    "<details>\n",
    "<summary style=\"color:green;\">Hint 1</summary>\n",
    "\n",
    "Start by tokenizing the query. You will need to call the <code>tokenize</code> function to split the query into manageable parts. Use the <code>bm25s.tokenize</code> function with the appropriate parameter.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color:green;\">Hint 2</summary>\n",
    "\n",
    "Make sure the corpus is indexed. This can be done by preparing the retriever with the document data before performing retrieval.\n",
    "Use the <code>.index</code> method of <code>BM25\\_RETRIEVER</code>.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color:green;\">Hint 3</summary>\n",
    "\n",
    "Use the BM25 retriever to calculate scores and retrieve documents. You’ll want to retrieve the top <code>k</code> documents.\n",
    "Remember that <code>BM25\\_RETRIEVER</code> has a method called <code>.retrieve</code>.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use these as a global defined BM25 retriever objects\n",
    "\n",
    "corpus = [x['title'] + \" \" + x['description'] for x in NEWS_DATA]\n",
    "BM25_RETRIEVER = bm25s.BM25(corpus=corpus)\n",
    "TOKENIZED_DATA = bm25s.tokenize(corpus)\n",
    "BM25_RETRIEVER.index(TOKENIZED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL\n",
    "\n",
    "def bm25_retrieve(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieves the top k relevant documents for a given query using the BM25 algorithm.\n",
    "\n",
    "    This function tokenizes the input query and uses a pre-indexed BM25 retriever to\n",
    "    search through a collection of documents. It returns the indices of the top k documents\n",
    "    that are most relevant to the query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query for which documents need to be retrieved.\n",
    "        top_k (int): The number of top relevant documents to retrieve. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of indices corresponding to the top k relevant documents\n",
    "        within the corpus.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Tokenize the query using the 'tokenize' function from the 'bm25s' module\n",
    "    tokenized_query = bm25s.tokenize(query)\n",
    "\n",
    "    # Index the tokenized chunks with the retriever\n",
    "    BM25_RETRIEVER.index(TOKENIZED_DATA)\n",
    "    \n",
    "    # Use the 'BM25_RETRIEVER' to retrieve documents and their scores based on the tokenized query\n",
    "    # Retrieve the top 'k' documents\n",
    "    results, scores = BM25_RETRIEVER.retrieve(tokenized_query, k=top_k)\n",
    "\n",
    "    # Extract the first element from 'results' to get the list of retrieved documents\n",
    "    results = results[0]\n",
    "\n",
    "    # Convert the retrieved documents into their corresponding indices in the results list\n",
    "    top_k_indices = [corpus.index(doc) for doc in results[:top_k]]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return top_k_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[752, 673, 289, 626, 43]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output is a list of indices\n",
    "bm25_retrieve(\"What are the recent news about GDP?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "[752, 673, 289, 626, 43]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "unittests.test_bm25_retrieve(bm25_retrieve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-3'></a>\n",
    "### 3.3 Semantic Search\n",
    "\n",
    "Semantic search enhances traditional search by focusing on the meaning behind queries, rather than just matching keywords. The idea is to convert the sentences into vectors that preserve semantic relations and then use metrics to compare them.\n",
    "\n",
    "<a id='3-4'></a>\n",
    "### 3.4 Embeddings\n",
    "\n",
    "A key component of semantic search is the use of embeddings, which are vector representations of text. These embeddings capture semantic meaning, allowing us to compare text based on context. One common way to measure the similarity between these vectors is through cosine similarity, which calculates how close two vectors are in high-dimensional space. This approach helps in finding content that is contextually similar to the user's query, leading to more accurate and meaningful search results.\n",
    "\n",
    "We've pre-embedded the corpus for you, so you will just load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-computed embeddings with joblib\n",
    "EMBEDDINGS = joblib.load(\"embeddings.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the sentence_transformers library to load an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = os.path.join(os.environ['MODEL_PATH'],\"BAAI/bge-base-en-v1.5\" )\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00886302, -0.04775146, -0.00156089,  0.01309993, -0.00206938,\n",
       "       -0.06157268,  0.01384688,  0.00101498, -0.04903949, -0.04762559,\n",
       "       -0.03628184,  0.00478035, -0.03492182,  0.05323148,  0.02193964,\n",
       "        0.03645132,  0.04029363, -0.00453639,  0.01883798, -0.03367384,\n",
       "        0.02516192, -0.04843621, -0.04047944,  0.02590903,  0.02175229,\n",
       "        0.03160364,  0.03937921, -0.03640463, -0.03113303, -0.01247228,\n",
       "        0.03661649, -0.00458202, -0.00100169, -0.03188789,  0.02957137,\n",
       "        0.01986158, -0.00737474,  0.02370178, -0.02151621, -0.07361361],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"RAG is awesome\"\n",
    "# Using, but truncating the result to not pollute the output, don't truncate it in the exercise.\n",
    "model.encode(query)[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of cosine similarity and embedding\n",
    "\n",
    "Let's see an example on using the cosine similarity. The function is the same used in the ungraded lab. You might check them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'What are the primary colors' and 'Yellow, red and blue' = 0.7377141714096069\n",
      "Similarity between 'What are the primary colors' and 'Cats are friendly animals' = 0.4508620798587799\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What are the primary colors\"\n",
    "query2 = \"Yellow, red and blue\"\n",
    "query3 = \"Cats are friendly animals\"\n",
    "\n",
    "query1_embed = model.encode(query1)\n",
    "query2_embed = model.encode(query2)\n",
    "query3_embed = model.encode(query3)\n",
    "\n",
    "print(f\"Similarity between '{query1}' and '{query2}' = {cosine_similarity(query1_embed, query2_embed)[0]}\")\n",
    "print(f\"Similarity between '{query1}' and '{query3}' = {cosine_similarity(query1_embed, query3_embed)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION!**: The output of `cosine_similarity` is always a list with the similarities between the vector (first input) and the vector/array of vectors (second output)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with the full embedding\n",
    "\n",
    "Let's have an example with the entire embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350 176]\n"
     ]
    }
   ],
   "source": [
    "query = \"Taylor Swift\"\n",
    "query_embed = model.encode(query)\n",
    "# The result is a matrix with one matrix per sample. Since there is only one sample (the query), it is a matrix with one matrix within.\n",
    "# This is why you need to get the first element\n",
    "similarity_scores = cosine_similarity(query_embed, EMBEDDINGS)\n",
    "similarity_indices = np.argsort(-similarity_scores) # Sort on decreasing order (sort the negative on increasing order), but return the indices\n",
    "# Top 2 indices\n",
    "top_2_indices = similarity_indices[:2]\n",
    "print(top_2_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'guid': '927257674585bb6ef669cf2c2f409fa7',\n",
       "  'title': '‘The working class can’t afford it’: the shocking truth about the money bands make on tour',\n",
       "  'description': 'As Taylor Swift tops $1bn in tour revenue, musicians playing smaller venues are facing pitiful fees and frequent losses. Should the state step in to save our live music scene?When you see a band playing to thousands of fans in a sun-drenched festival field, signing a record deal with a major label or playing endlessly from the airwaves, it’s easy to conjure an image of success that comes with some serious cash to boot – particularly when Taylor Swift has broken $1bn in revenue for her current Eras tour. But looks can be deceiving. “I don’t blame the public for seeing a band playing to 2,000 people and thinking they’re minted,” says artist manager Dan Potts. “But the reality is quite different.”Post-Covid there has been significant focus on grassroots music venues as they struggle to stay open. There’s been less focus on the actual ability of artists to tour these venues. David Martin, chief executive officer of the Featured Artists Coalition (FAC), says we’re in a “cost-of-touring crisis”. Pretty much every cost attached to touring – van hire, crew, travel, accommodation, food and drink – has gone up, while fees and audiences often have not. “[Playing] live is becoming financially unsustainable for many artists,” he says. “Artists are seeing [playing] live as a loss leader now. That’s if they can even afford to make it work in the first place.” Continue reading...',\n",
       "  'venue': 'The Guardian',\n",
       "  'url': 'https://www.theguardian.com/music/2024/apr/25/shocking-truth-money-bands-make-on-tour-taylor-swift',\n",
       "  'published_at': '2024-04-25',\n",
       "  'updated_at': '2024-04-26'},\n",
       " {'guid': 'e8712c1d69d29f0ca802f07d9b60ffe7',\n",
       "  'title': 'Estate of Tupac Shakur threatens legal action against Drake over AI diss track',\n",
       "  'description': 'Drake used AI to simulate the voice of the late rapper and have him chide Kendrick Lamar, which the estate calls a ‘flagrant violation’The estate of the late Tupac Shakur has sent a cease and desist letter to Drake, following the release of a Drake track that uses an AI version of Shakur’s voice to lambast Kendrick Lamar.As seen by Billboard, the letter instructs Drake to remove the track, Taylor Made Freestyle, within 24 hours, or face legal action. Continue reading...',\n",
       "  'venue': 'The Guardian',\n",
       "  'url': 'https://www.theguardian.com/music/2024/apr/25/estate-of-tupac-shakur-threatens-legal-action-against-drake-over-ai-diss-track',\n",
       "  'published_at': '2024-04-25',\n",
       "  'updated_at': '2024-04-26'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving the data\n",
    "query_news(top_2_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex02'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Now it's time to build the `semantic_search_retrieve` function! You will use embeddings to represent the query and then apply the `cosine_similarity` function to compute how similar the query is to each document in the embedding matrix. The goal is to retrieve the indices of the top_k most similar documents by ordering the similarity scores in descending order.\n",
    "\n",
    "In this exercise, you will explore how embeddings and cosine similarity can be used in a semantic search to effectively find contextually relevant documents.\n",
    "\n",
    "<details>\n",
    "<summary style=\"color:green;\">Hint 1</summary>\n",
    "\n",
    "Start by encoding the query into an embedding using the pre-trained model. Remember that the call is <code>model.encode(query)</code>.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color:green;\">Hint 2</summary>\n",
    "\n",
    "Calculate the cosine similarity between the query embedding and all document embeddings. This will provide a set of similarity scores. You might write:  \n",
    "<code>similarity_scores = cosine_similarity(...)</code>  \n",
    "Make sure to pass the query embedding and document embeddings correctly and remember that the output of the function is a list with all the scores.\n",
    "\n",
    "</details> \n",
    "\n",
    "<details>\n",
    "<summary style=\"color:green;\">Hint 3</summary>\n",
    "\n",
    "Sort the similarity scores to find the order of most relevant documents. You will need to use:  \n",
    "<code>similarity_indices = np.argsort(...)</code>, keep in mind that by default it sorts in ascending order, you need it in descending order! Check how it was done in the previous examples if you are not sure how to proceed. There is no unique way of doing it.\n",
    "Then slice to get the top-k indices and convert them to integers.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "\n",
    "def semantic_search_retrieve(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top k relevant documents for a given query using semantic search and cosine similarity.\n",
    "\n",
    "    This function generates an embedding for the input query and compares it against pre-computed document\n",
    "    embeddings using cosine similarity. The indices of the top k most similar documents are returned.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query for which relevant documents need to be retrieved.\n",
    "        top_k (int): The number of top relevant documents to retrieve. Default value is 5.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of indices corresponding to the top k most relevant documents in the corpus.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # Generate the embedding for the query using the pre-trained model\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    # Calculate the cosine similarity scores between the query embedding and the pre-computed document embeddings\n",
    "    similarity_scores = cosine_similarity(query_embedding, EMBEDDINGS)\n",
    "    \n",
    "    # Sort the similarity scores in descending order and get the indices\n",
    "    similarity_indices = np.argsort(-similarity_scores)\n",
    "\n",
    "    # Select the indices of the top k documents as a numpy array\n",
    "    top_k_indices_array = similarity_indices[:top_k]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cast them to int \n",
    "    top_k_indices = [int(x) for x in top_k_indices_array]\n",
    "    \n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[743, 673, 626, 752, 326]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see an example\n",
    "semantic_search_retrieve(\"What are the recent news about GDP?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "[743, 673, 626, 752, 326]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "unittests.test_semantic_search_retrieve(semantic_search_retrieve, EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-5'></a>\n",
    "### 3.5 RRF Retrieve\n",
    "\n",
    "Reciprocal Rank Fusion (RRF) is an information retrieval technique used to combine results from multiple ranking systems. It aims to enhance the overall retrieval performance by integrating different ranking algorithms. RRF assigns a score to each document based on its rank in different result lists, allowing it to leverage the strengths of several retrieval approaches.\n",
    "\n",
    "#### Formula\n",
    "\n",
    "The RRF formula for computing the score of a document $d$ is:\n",
    "\n",
    "$$ \n",
    "\\text{Score}(d) = \\sum_{r=1}^{n} \\frac{1}{k + \\text{rank}_r(d)} \n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of ranking systems,\n",
    "- $\\text{rank}_r(d)$ is the rank of document $d$ in the $r$-th result list,\n",
    "- $k$ is a constant to scale the contribution of each rank, often set to a small positive value.\n",
    "\n",
    "The resulting RRF score is higher for documents that appear with high rankings across multiple systems, helping to combine different retrieval methodologies effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex03'></a>\n",
    "### Exercise 3\n",
    "\n",
    "In this exercise, you will implement the `reciprocal_rank_fusion` function. This function will take four parameters:\n",
    "- `list1` and `list2`, which are lists of indices representing the top-ranked documents from two different retrieval systems.\n",
    "- `top_k`, which is the number of top relevant indices you wish to retrieve after fusion. \n",
    "- `K`, a constant used in the Reciprocal Rank Fusion (RRF) formula to scale the influence of rank position.\n",
    "\n",
    "Your task is to use the RRF algorithm to merge rankings from the two lists and output the indices of the top-k documents as determined by the combined RRF scores. This exercise will help you understand how RRF works to aggregate results from multiple retrieval systems, improving the overall search performance.\n",
    "\n",
    "To complete this exercise, you will need an understanding of how to iterate over lists, calculate Reciprocal Rank scores, and effectively combine ranked results.\n",
    "\n",
    "<details>\n",
    "<summary style=\"color:green;\">Hint 1</summary>\n",
    "\n",
    "Begin by creating an empty dictionary to store RRF scores, mapping each document index to a cumulative score. Initialize it with:  \n",
    "<code>rrf_scores = {}</code>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color:green;\">Hint 2</summary>\n",
    "\n",
    "Iterate through each list and calculate scores. For each item, if it’s not already in the dictionary, add it with an initial score. Update the score by considering the rank and constant K. Look at:  \n",
    "<code>if item not in rrf_scores:</code>  \n",
    "<code>rrf_scores[item] = ...</code>  \n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style=\"color:green;\">Hint 3</summary>\n",
    "\n",
    "After computing scores, sort the indices by their RRF scores in descending order to get the most relevant ones. You need to select the top results with:  \n",
    "<code>sorted_items = sorted(rrf_scores, key=rrf_scores.get, reverse=True)</code>  \n",
    "Then limit this to the top-k results by slicing:  \n",
    "<code>top_k_indices = [... for ... in sorted_items[:top_k]]</code>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "def reciprocal_rank_fusion(list1, list2, top_k=5, K=60):\n",
    "    \"\"\"\n",
    "    Fuse rank from multiple IR systems using Reciprocal Rank Fusion.\n",
    "\n",
    "    Args:\n",
    "        list1 (list[int]): A list of indices of the top-k documents that match the query.\n",
    "        list2 (list[int]): Another list of indices of the top-k documents that match the query.\n",
    "        top_k (int): The number of top documents to consider from each list for fusion. Defaults to 5.\n",
    "        K (int): A constant used in the RRF formula. Defaults to 60.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of indices of the top-k documents sorted by their RRF scores.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Create a dictionary to store the RRF scores for each document index\n",
    "    rrf_scores = {}\n",
    "\n",
    "    # Iterate over each document list\n",
    "    for lst in [list1, list2]:\n",
    "        # Calculate the RRF score for each document index\n",
    "        for rank, item in enumerate(lst, start=1): # Start = 1 set the first element as 1 and not 0. \n",
    "                                                   # This is a convention on how ranks work (the first element in ranking is denoted by 1 and not 0 as in lists)\n",
    "            # If the item is not in the dictionary, initialize its score to 0\n",
    "            if item not in rrf_scores:\n",
    "                rrf_scores[item] = 0\n",
    "            # Update the RRF score for each document index using the formula 1 / (rank + K)\n",
    "            rrf_scores[item] += (1/(rank+K))\n",
    "\n",
    "    # Sort the document indices based on their RRF scores in descending order\n",
    "    sorted_items = sorted(rrf_scores.items(), key=lambda x: x[1], reverse = True)\n",
    "\n",
    "    # Slice the list to get the top-k document indices\n",
    "    top_k_indices = [int(x[0]) for x in sorted_items[:top_k]]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search List: [743, 673, 626, 752, 326]\n",
      "BM25 List: [752, 673, 289, 626, 43]\n",
      "RRF List: [673, 752, 626, 743, 289]\n"
     ]
    }
   ],
   "source": [
    "list1 = semantic_search_retrieve('What are the recent news about GDP?')\n",
    "list2 = bm25_retrieve('What are the recent news about GDP?')\n",
    "rrf_list = reciprocal_rank_fusion(list1, list2)\n",
    "print(f\"Semantic Search List: {list1}\")\n",
    "print(f\"BM25 List: {list2}\")\n",
    "print(f\"RRF List: {rrf_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output (order may vary)**\n",
    "```\n",
    "Semantic Search List: [743 673 626 752 326]\n",
    "BM25 List: [752, 673, 289, 626, 43]\n",
    "RRF List: [673, 752, 626, 743, 289]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "unittests.test_reciprocal_rank_fusion(reciprocal_rank_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Completing the RAG System\n",
    "\n",
    "<a id='4-1'></a>\n",
    "### 4.1 Creating the final prompt\n",
    "\n",
    "Now you will proceed as you proceeded in the previous assignment. These functions are the same you wrote in the previous assignment, but adjusted to fit in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_final_prompt(query, top_k, retrieve_function = None, use_rag=True):\n",
    "    \"\"\"\n",
    "    Generates an augmented prompt for a Retrieval-Augmented Generation (RAG) system by retrieving the top_k most \n",
    "    relevant documents based on a given query.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The search query for which the relevant documents are to be retrieved.\n",
    "    top_k (int): The number of top relevant documents to retrieve.\n",
    "    retrieve_function (callable): The function used to retrieve relevant documents. If 'reciprocal_rank_fusion', \n",
    "                                  it will combine results from different retrieval functions.\n",
    "    use_rag (bool): A flag to determine whether to incorporate retrieved data into the prompt (default is True).\n",
    "\n",
    "    Returns:\n",
    "    str: A prompt that includes the top_k relevant documents formatted for use in a RAG system.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the prompt as the initial query\n",
    "    prompt = query\n",
    "    \n",
    "    # If not using rag, return the prompt\n",
    "    if not use_rag:\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    # Determine which retrieve function to use based on its name.\n",
    "    if retrieve_function.__name__ == 'reciprocal_rank_fusion':\n",
    "        # Retrieve top documents using two different methods.\n",
    "        list1 = semantic_search_retrieve(query, top_k)\n",
    "        list2 = bm25_retrieve(query, top_k)\n",
    "        # Combine the results using reciprocal rank fusion.\n",
    "        top_k_indices = retrieve_function(list1, list2, top_k)\n",
    "    else:\n",
    "        # Use the provided retrieval function.\n",
    "        top_k_indices = retrieve_function(query=query, top_k=top_k)\n",
    "    \n",
    "    \n",
    "    # Retrieve documents from the dataset using the indices.\n",
    "    relevant_documents = query_news(top_k_indices)\n",
    "    \n",
    "    formatted_documents = []\n",
    "\n",
    "    # Iterate over each retrieved document.\n",
    "    for document in relevant_documents:\n",
    "        # Format each document into a structured string.\n",
    "        formatted_document = (\n",
    "            f\"Title: {document['title']}, Description: {document['description']}, \"\n",
    "            f\"Published at: {document['published_at']}\\nURL: {document['url']}\"\n",
    "        )\n",
    "        # Append the formatted string to the main data string with a newline for separation.\n",
    "        formatted_documents.append(formatted_document)\n",
    "\n",
    "    retrieve_data_formatted = \"\\n\".join(formatted_documents)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Answer the user query below. There will be provided additional information for you to compose your answer. \"\n",
    "        f\"The relevant information provided is from 2024 and it should be added as your overall knowledge to answer the query, \"\n",
    "        f\"you should not rely only on this information to answer the query, but add it to your overall knowledge.\"\n",
    "        f\"Query: {query}\\n\"\n",
    "        f\"2024 News: {retrieve_data_formatted}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def llm_call(query, retrieve_function = None, top_k = 5,use_rag = True):\n",
    "\n",
    "    # Get the system and user dictionaries\n",
    "    prompt = generate_final_prompt(query, top_k = top_k, retrieve_function = retrieve_function, use_rag = use_rag)\n",
    "\n",
    "    generated_response = generate_with_single_input(prompt)\n",
    "\n",
    "    generated_message = generated_response['content']\n",
    "    \n",
    "    return generated_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "query = \"Recent news in technology. Provide sources.\"\n",
    "print(llm_call(query, retrieve_function = semantic_search_retrieve))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-2'></a>\n",
    "### 4.2 Experimenting with the RAG system\n",
    "\n",
    "Now it is time to test our RAG system! Run the code to generate a widget that will output 4 responses for each query using the following methods:\n",
    "\n",
    "- RAG with Semantic Search\n",
    "- RAG with BM25\n",
    "- RAG with Reciprocal Rank Fusion\n",
    "- Without RAG\n",
    "\n",
    "You may use one of these questions to test, but feel free to ask your own!\n",
    "\n",
    "* What were the most important events of the past year?\n",
    "* How is global warming progressing in 2024?\n",
    "* Tell me about the most recent advances in AI.\n",
    "* Give me the most important facts from past year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "display_widget(llm_call, semantic_search_retrieve, bm25_retrieve, reciprocal_rank_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-3'></a>\n",
    "### 4.3 Ask yourself\n",
    "\n",
    "In your opinion, which setup gave better results? Is there a type of query where one method outperforms the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You finished your second assignment. Keep it up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
