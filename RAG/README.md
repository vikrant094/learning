#Retrieval Augmented Generation (RAG)

Retrieval Augmented Generation (RAG) improves large language model (LLM) responses by retrieving relevant data from knowledge bases—often private, recent, or domain-specific—and using it to generate more accurate, grounded answers.

During this learing I have learned how to build RAG systems that connect LLMs to external data sources, explored core components like retrievers, vector databases, and language models, and apply key techniques at both the component and system level. 
Through hands-on work with real production tools, I have gained the skills to design, refine, and evaluate reliable RAG pipelines—and adapt to new methods as the field advances.


Through hands-on code, i have:

- Build RAG system by writing retrieval and prompt augmentation functions and passing structured input into an LLM.
- Implement and compare retrieval methods like semantic search, BM25, and Reciprocal Rank Fusion to see how each impacts LLM responses.
- Scale RAG system using Weaviate and a real news dataset—chunking, indexing, and retrieving documents with a vector database.
- Develop a domain-specific chatbot for a fictional clothing store that answers FAQs and provides product suggestions based on a custom dataset.
- Improve chatbot reliability by handling real-world challenges like dynamic pricing and logging user interactions for monitoring and debugging.
- Develop a domain-specific chatbot using open-source LLMs hosted by Together AI for a fictional clothing store that answers FAQs and provides product suggestions based on a custom dataset.
